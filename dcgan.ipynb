{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dcgan.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"H7yOkW50PrwA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":222},"outputId":"56f42cd3-a055-41fb-c649-b454d26e84fc","executionInfo":{"status":"ok","timestamp":1554698666346,"user_tz":-480,"elapsed":66243,"user":{"displayName":"bruce drake","photoUrl":"","userId":"11355858952103147079"}}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":10,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 131304 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu1~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.3-0ubuntu1~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"metadata":{"id":"jrZiDz9iQyDP","colab_type":"code","colab":{}},"cell_type":"code","source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HHu87hVkQ5Vv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"47f93ea8-7d11-4600-ec1c-1c06c433027d","executionInfo":{"status":"ok","timestamp":1554698756694,"user_tz":-480,"elapsed":1117,"user":{"displayName":"bruce drake","photoUrl":"","userId":"11355858952103147079"}}},"cell_type":"code","source":["cd drive/Projects/DCGAN"],"execution_count":13,"outputs":[{"output_type":"stream","text":["/content/drive/Projects/DCGAN\n"],"name":"stdout"}]},{"metadata":{"id":"HiJycP_lRHzU","colab_type":"code","colab":{}},"cell_type":"code","source":["!mkdir images"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tnUPey_BRUdH","colab_type":"code","colab":{}},"cell_type":"code","source":["!mkdir models_and_weights"],"execution_count":0,"outputs":[]},{"metadata":{"id":"j8m4QsFy4i4k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":373},"outputId":"20a636b0-9ef4-4a7f-f76f-6ed354619872","executionInfo":{"status":"ok","timestamp":1554698794973,"user_tz":-480,"elapsed":4814,"user":{"displayName":"bruce drake","photoUrl":"","userId":"11355858952103147079"}}},"cell_type":"code","source":["!pip install tensorflow-gpu"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (1.13.1)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.14.6)\n","Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.13.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.11.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.7.1)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.7)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.33.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.15.0)\n","Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.13.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.9)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.7.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (3.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu) (0.15.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu) (40.9.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.8.0)\n","Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu) (2.0.0)\n","Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu) (5.1.3)\n"],"name":"stdout"}]},{"metadata":{"id":"ZuLfBbk54ntT","colab_type":"code","colab":{}},"cell_type":"code","source":["from tensorflow.contrib.keras.api.keras.datasets import mnist\n","from tensorflow.contrib.keras.api.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n","from tensorflow.contrib.keras.api.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n","from tensorflow.contrib.keras.api.keras.layers import LeakyReLU\n","from tensorflow.contrib.keras.api.keras.layers import UpSampling2D, Conv2D\n","from tensorflow.contrib.keras.api.keras.models import Sequential, Model\n","from tensorflow.contrib.keras.api.keras.optimizers import Adam\n","from tensorflow.examples.tutorials.mnist import input_data\n","import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","\n","\n","\n","weight_path=\"./models_and_weights/\"\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"75JFj5JU40sW","colab_type":"code","colab":{}},"cell_type":"code","source":["class DCGAN():\n","    def __init__(self):\n","        # Input shape\n","        self.img_rows = 28\n","        self.img_cols = 28\n","        self.channels = 1\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","        self.latent_dim = 100\n","\n","        optimizer = Adam(0.0002, 0.5)\n","\n","        # Build and compile the discriminator\n","        self.discriminator = self.build_discriminator()\n","        \n","\n","        # Build the generator\n","        self.generator = self.build_generator()\n","\n","        # load weight\n","        self.load_weights_from_file()\n","        \n","        # Compile the discriminator\n","        self.discriminator.compile(loss='binary_crossentropy',\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","        \n","        # The generator takes noise as input and generates imgs\n","        z = Input(shape=(self.latent_dim,))\n","        img = self.generator(z)\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","    \n","        # The discriminator takes generated images as input and determines validity\n","        valid = self.discriminator(img)\n","        \n","        # The combined model  (stacked generator and discriminator)\n","        # Trains the generator to fool the discriminator\n","        self.combined = Model(z, valid)\n","        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","    def build_generator(self):\n","\n","        model = Sequential()\n","\n","        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n","        model.add(Reshape((7, 7, 128)))\n","        model.add(UpSampling2D())\n","        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Activation(\"relu\"))\n","        model.add(UpSampling2D())\n","        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Activation(\"relu\"))\n","        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n","        model.add(Activation(\"tanh\"))\n","\n","        model.summary()\n","\n","        noise = Input(shape=(self.latent_dim,))\n","        img = model(noise)\n","\n","        return Model(noise, img)\n","\n","    def build_discriminator(self):\n","\n","        model = Sequential()\n","\n","        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n","        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        model.add(Flatten())\n","        model.add(Dense(1, activation='sigmoid'))\n","\n","        model.summary()\n","\n","        img = Input(shape=self.img_shape)\n","        validity = model(img)\n","\n","        return Model(img, validity)\n","\n","    def train(self, epochs, batch_size=128, save_interval=50):\n","\n","        # Load the dataset\n","        (X_train, _), (_, _) = mnist.load_data()\n","#         (X_train, y_train) = mnist.train.images, mnist.train.labels\n","        # Rescale -1 to 1\n","        X_train = X_train / 127.5 - 1.\n","        print(X_train.shape)\n","        X_train = np.reshape(X_train,(-1,28,28,1))\n","\n","        # Adversarial ground truths\n","        valid = np.ones((batch_size, 1))\n","        fake = np.zeros((batch_size, 1))\n","        \n","        for epoch in range(epochs):\n","\n","            # ---------------------\n","            #  Train Discriminator\n","            # ---------------------\n","\n","            # Select a random half of images\n","            idx = np.random.randint(0, X_train.shape[0], batch_size)\n","            imgs = X_train[idx]\n","\n","            # Sample noise and generate a batch of new images\n","            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","            gen_imgs = self.generator.predict(noise)\n","\n","            # Train the discriminator (real classified as ones and generated as zeros)\n","            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n","            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","            # ---------------------\n","            #  Train Generator\n","            # ---------------------\n","\n","            # Train the generator (wants discriminator to mistake images as real)\n","            g_loss = self.combined.train_on_batch(noise, valid)\n","\n","            # Plot the progress\n","            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n","\n","            # If at save interval => save generated image samples\n","            if epoch % save_interval == 0:\n","                self.save_imgs(epoch)\n","                self.save_model()\n","\n","    def save_imgs(self, epoch):\n","        r, c = 5, 5\n","        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n","        gen_imgs = self.generator.predict(noise)\n","\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        fig.savefig(\"images/mnist_%d.png\" % epoch)\n","        plt.close()\n","        return\n","      \n","      # Save model\n","    def save_model(self):\n","        self.discriminator.save_weights(weight_path+'dcgan_discriminator_weight.h5')\n","        self.generator.save_weights(weight_path+'dcgan_generator_weight.h5')\n","        print(\"save model\")\n","        \n","      # load weights  \n","    def load_weights_from_file(self):\n","        # discriminator weight\n","        if self.discriminator != None and os.path.isfile(weight_path+\"dcgan_discriminator_weight.h5\"):\n","          print(\"discriminator_weights exists\")\n","          self.discriminator.load_weights(weight_path+\"dcgan_discriminator_weight.h5\")\n","        # generator weight   \n","        if self.generator != None and os.path.isfile(weight_path+\"dcgan_generator_weight.h5\"):\n","          print(\"generator_weights exists\")\n","          self.generator.load_weights(weight_path+\"dcgan_generator_weight.h5\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"o0UJ23ms5MA7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"513f70cb-f643-450c-82b6-728d7e5c012b","executionInfo":{"status":"ok","timestamp":1554700326418,"user_tz":-480,"elapsed":3672,"user":{"displayName":"bruce drake","photoUrl":"","userId":"11355858952103147079"}}},"cell_type":"code","source":["!mkdir images"],"execution_count":29,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘images’: File exists\n"],"name":"stdout"}]},{"metadata":{"id":"KHPVgmV-5ZdS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"808e8d74-cc59-4626-905d-786929855c9f","executionInfo":{"status":"ok","timestamp":1554700637066,"user_tz":-480,"elapsed":1161,"user":{"displayName":"bruce drake","photoUrl":"","userId":"11355858952103147079"}}},"cell_type":"code","source":["print(os.path.isfile(weight_path+\"dcgan_generator_weight.h5\"))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"metadata":{"id":"Jok4y6ZU6JTf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52231},"outputId":"a771da99-38bd-4907-9380-950a0de2d90d","executionInfo":{"status":"ok","timestamp":1554703079131,"user_tz":-480,"elapsed":2218101,"user":{"displayName":"bruce drake","photoUrl":"","userId":"11355858952103147079"}}},"cell_type":"code","source":["dcgan = DCGAN()\n","dcgan.train(epochs=3000, batch_size=1024, save_interval=100)\n","dcgan.save_model()"],"execution_count":36,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_70 (Conv2D)           (None, 14, 14, 32)        320       \n","_________________________________________________________________\n","leaky_re_lu_40 (LeakyReLU)   (None, 14, 14, 32)        0         \n","_________________________________________________________________\n","dropout_40 (Dropout)         (None, 14, 14, 32)        0         \n","_________________________________________________________________\n","conv2d_71 (Conv2D)           (None, 7, 7, 64)          18496     \n","_________________________________________________________________\n","zero_padding2d_10 (ZeroPaddi (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","batch_normalization_v1_50 (B (None, 8, 8, 64)          256       \n","_________________________________________________________________\n","leaky_re_lu_41 (LeakyReLU)   (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","dropout_41 (Dropout)         (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","conv2d_72 (Conv2D)           (None, 4, 4, 128)         73856     \n","_________________________________________________________________\n","batch_normalization_v1_51 (B (None, 4, 4, 128)         512       \n","_________________________________________________________________\n","leaky_re_lu_42 (LeakyReLU)   (None, 4, 4, 128)         0         \n","_________________________________________________________________\n","dropout_42 (Dropout)         (None, 4, 4, 128)         0         \n","_________________________________________________________________\n","conv2d_73 (Conv2D)           (None, 4, 4, 256)         295168    \n","_________________________________________________________________\n","batch_normalization_v1_52 (B (None, 4, 4, 256)         1024      \n","_________________________________________________________________\n","leaky_re_lu_43 (LeakyReLU)   (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","dropout_43 (Dropout)         (None, 4, 4, 256)         0         \n","_________________________________________________________________\n","flatten_10 (Flatten)         (None, 4096)              0         \n","_________________________________________________________________\n","dense_20 (Dense)             (None, 1)                 4097      \n","=================================================================\n","Total params: 393,729\n","Trainable params: 392,833\n","Non-trainable params: 896\n","_________________________________________________________________\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_21 (Dense)             (None, 6272)              633472    \n","_________________________________________________________________\n","reshape_10 (Reshape)         (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","up_sampling2d_20 (UpSampling (None, 14, 14, 128)       0         \n","_________________________________________________________________\n","conv2d_74 (Conv2D)           (None, 14, 14, 128)       147584    \n","_________________________________________________________________\n","batch_normalization_v1_53 (B (None, 14, 14, 128)       512       \n","_________________________________________________________________\n","activation_30 (Activation)   (None, 14, 14, 128)       0         \n","_________________________________________________________________\n","up_sampling2d_21 (UpSampling (None, 28, 28, 128)       0         \n","_________________________________________________________________\n","conv2d_75 (Conv2D)           (None, 28, 28, 64)        73792     \n","_________________________________________________________________\n","batch_normalization_v1_54 (B (None, 28, 28, 64)        256       \n","_________________________________________________________________\n","activation_31 (Activation)   (None, 28, 28, 64)        0         \n","_________________________________________________________________\n","conv2d_76 (Conv2D)           (None, 28, 28, 1)         577       \n","_________________________________________________________________\n","activation_32 (Activation)   (None, 28, 28, 1)         0         \n","=================================================================\n","Total params: 856,193\n","Trainable params: 855,809\n","Non-trainable params: 384\n","_________________________________________________________________\n","discriminator_weights exists\n","generator_weights exists\n","(60000, 28, 28)\n","0 [D loss: 0.987530, acc.: 28.27%] [G loss: 0.589384]\n","save model\n","1 [D loss: 0.784686, acc.: 49.66%] [G loss: 0.770107]\n","2 [D loss: 0.751493, acc.: 47.71%] [G loss: 0.976982]\n","3 [D loss: 0.792562, acc.: 42.87%] [G loss: 0.958842]\n","4 [D loss: 0.772902, acc.: 46.58%] [G loss: 0.898242]\n","5 [D loss: 0.808382, acc.: 42.63%] [G loss: 0.892776]\n","6 [D loss: 0.804771, acc.: 42.09%] [G loss: 0.923165]\n","7 [D loss: 0.804488, acc.: 42.04%] [G loss: 0.921358]\n","8 [D loss: 0.784408, acc.: 45.26%] [G loss: 0.915218]\n","9 [D loss: 0.796952, acc.: 42.48%] [G loss: 0.915626]\n","10 [D loss: 0.778164, acc.: 45.26%] [G loss: 0.947625]\n","11 [D loss: 0.789505, acc.: 43.85%] [G loss: 0.926082]\n","12 [D loss: 0.785580, acc.: 44.24%] [G loss: 0.916262]\n","13 [D loss: 0.793284, acc.: 43.41%] [G loss: 0.903591]\n","14 [D loss: 0.810776, acc.: 40.77%] [G loss: 0.960337]\n","15 [D loss: 0.785529, acc.: 44.58%] [G loss: 0.910796]\n","16 [D loss: 0.762991, acc.: 48.63%] [G loss: 0.950164]\n","17 [D loss: 0.800426, acc.: 42.58%] [G loss: 0.897421]\n","18 [D loss: 0.785432, acc.: 44.53%] [G loss: 0.931621]\n","19 [D loss: 0.780456, acc.: 45.56%] [G loss: 0.962695]\n","20 [D loss: 0.791668, acc.: 44.24%] [G loss: 0.912765]\n","21 [D loss: 0.757303, acc.: 47.56%] [G loss: 0.890490]\n","22 [D loss: 0.799250, acc.: 43.51%] [G loss: 0.890704]\n","23 [D loss: 0.795400, acc.: 43.70%] [G loss: 0.928464]\n","24 [D loss: 0.792169, acc.: 44.58%] [G loss: 0.957245]\n","25 [D loss: 0.781437, acc.: 45.65%] [G loss: 0.885415]\n","26 [D loss: 0.779283, acc.: 45.36%] [G loss: 0.914989]\n","27 [D loss: 0.779078, acc.: 45.46%] [G loss: 0.922851]\n","28 [D loss: 0.762867, acc.: 46.88%] [G loss: 0.949076]\n","29 [D loss: 0.777027, acc.: 43.95%] [G loss: 0.951007]\n","30 [D loss: 0.800914, acc.: 43.90%] [G loss: 0.900216]\n","31 [D loss: 0.767950, acc.: 45.80%] [G loss: 0.891354]\n","32 [D loss: 0.794101, acc.: 43.65%] [G loss: 0.889196]\n","33 [D loss: 0.806714, acc.: 41.70%] [G loss: 0.906846]\n","34 [D loss: 0.792318, acc.: 43.80%] [G loss: 0.961992]\n","35 [D loss: 0.785061, acc.: 45.26%] [G loss: 0.903811]\n","36 [D loss: 0.741071, acc.: 49.02%] [G loss: 0.886336]\n","37 [D loss: 0.789503, acc.: 44.34%] [G loss: 0.883983]\n","38 [D loss: 0.778889, acc.: 44.97%] [G loss: 0.939569]\n","39 [D loss: 0.766943, acc.: 45.85%] [G loss: 0.987293]\n","40 [D loss: 0.776806, acc.: 45.75%] [G loss: 0.906503]\n","41 [D loss: 0.760291, acc.: 47.71%] [G loss: 0.872107]\n","42 [D loss: 0.787090, acc.: 44.73%] [G loss: 0.888352]\n","43 [D loss: 0.806178, acc.: 40.58%] [G loss: 0.895780]\n","44 [D loss: 0.779103, acc.: 45.26%] [G loss: 0.954497]\n","45 [D loss: 0.818461, acc.: 39.31%] [G loss: 0.913228]\n","46 [D loss: 0.777636, acc.: 45.70%] [G loss: 0.930817]\n","47 [D loss: 0.785038, acc.: 43.51%] [G loss: 0.962581]\n","48 [D loss: 0.759041, acc.: 46.97%] [G loss: 0.961303]\n","49 [D loss: 0.748549, acc.: 49.71%] [G loss: 0.967866]\n","50 [D loss: 0.799809, acc.: 42.24%] [G loss: 0.920142]\n","51 [D loss: 0.777122, acc.: 46.34%] [G loss: 0.916747]\n","52 [D loss: 0.811377, acc.: 41.70%] [G loss: 0.899410]\n","53 [D loss: 0.793599, acc.: 43.26%] [G loss: 0.923319]\n","54 [D loss: 0.792972, acc.: 43.65%] [G loss: 0.950854]\n","55 [D loss: 0.780569, acc.: 45.61%] [G loss: 0.920588]\n","56 [D loss: 0.749171, acc.: 48.78%] [G loss: 0.929435]\n","57 [D loss: 0.776270, acc.: 44.92%] [G loss: 0.931706]\n","58 [D loss: 0.764208, acc.: 47.17%] [G loss: 0.955358]\n","59 [D loss: 0.748345, acc.: 49.12%] [G loss: 0.970845]\n","60 [D loss: 0.801259, acc.: 42.63%] [G loss: 0.892712]\n","61 [D loss: 0.796002, acc.: 43.26%] [G loss: 0.903616]\n","62 [D loss: 0.791835, acc.: 42.63%] [G loss: 0.914060]\n","63 [D loss: 0.769041, acc.: 45.26%] [G loss: 0.919881]\n","64 [D loss: 0.737110, acc.: 49.80%] [G loss: 0.947074]\n","65 [D loss: 0.782298, acc.: 43.60%] [G loss: 0.903289]\n","66 [D loss: 0.782225, acc.: 44.34%] [G loss: 0.889489]\n","67 [D loss: 0.760465, acc.: 47.66%] [G loss: 0.936649]\n","68 [D loss: 0.774451, acc.: 46.19%] [G loss: 0.908435]\n","69 [D loss: 0.755994, acc.: 47.12%] [G loss: 0.938074]\n","70 [D loss: 0.758878, acc.: 48.68%] [G loss: 0.937263]\n","71 [D loss: 0.785134, acc.: 43.90%] [G loss: 0.895554]\n","72 [D loss: 0.765489, acc.: 46.44%] [G loss: 0.913981]\n","73 [D loss: 0.775496, acc.: 45.17%] [G loss: 0.929327]\n","74 [D loss: 0.750043, acc.: 47.85%] [G loss: 0.952811]\n","75 [D loss: 0.792524, acc.: 43.21%] [G loss: 0.939666]\n","76 [D loss: 0.771888, acc.: 46.44%] [G loss: 0.894930]\n","77 [D loss: 0.775173, acc.: 46.34%] [G loss: 0.901825]\n","78 [D loss: 0.784314, acc.: 42.38%] [G loss: 0.889979]\n","79 [D loss: 0.774323, acc.: 45.21%] [G loss: 0.905559]\n","80 [D loss: 0.780143, acc.: 44.04%] [G loss: 0.915000]\n","81 [D loss: 0.763135, acc.: 46.24%] [G loss: 0.903429]\n","82 [D loss: 0.753729, acc.: 48.24%] [G loss: 0.908423]\n","83 [D loss: 0.775537, acc.: 44.24%] [G loss: 0.899337]\n","84 [D loss: 0.758473, acc.: 48.97%] [G loss: 0.897926]\n","85 [D loss: 0.784702, acc.: 43.75%] [G loss: 0.929974]\n","86 [D loss: 0.768484, acc.: 45.41%] [G loss: 0.899280]\n","87 [D loss: 0.762279, acc.: 47.80%] [G loss: 0.919981]\n","88 [D loss: 0.793486, acc.: 43.26%] [G loss: 0.909563]\n","89 [D loss: 0.763396, acc.: 46.09%] [G loss: 0.925031]\n","90 [D loss: 0.784847, acc.: 43.60%] [G loss: 0.891327]\n","91 [D loss: 0.754984, acc.: 47.17%] [G loss: 0.909631]\n","92 [D loss: 0.800362, acc.: 42.72%] [G loss: 0.919225]\n","93 [D loss: 0.786675, acc.: 44.78%] [G loss: 0.900350]\n","94 [D loss: 0.786367, acc.: 42.24%] [G loss: 0.907385]\n","95 [D loss: 0.793672, acc.: 43.75%] [G loss: 0.877430]\n","96 [D loss: 0.787647, acc.: 43.07%] [G loss: 0.907130]\n","97 [D loss: 0.763519, acc.: 45.85%] [G loss: 0.907186]\n","98 [D loss: 0.770191, acc.: 46.73%] [G loss: 0.929354]\n","99 [D loss: 0.783074, acc.: 45.07%] [G loss: 0.875701]\n","100 [D loss: 0.777534, acc.: 44.48%] [G loss: 0.899919]\n","save model\n","101 [D loss: 0.810005, acc.: 41.21%] [G loss: 0.910532]\n","102 [D loss: 0.783469, acc.: 43.07%] [G loss: 0.888641]\n","103 [D loss: 0.792140, acc.: 44.09%] [G loss: 0.911691]\n","104 [D loss: 0.781293, acc.: 43.51%] [G loss: 0.928197]\n","105 [D loss: 0.783764, acc.: 42.63%] [G loss: 0.910420]\n","106 [D loss: 0.782346, acc.: 45.07%] [G loss: 0.913356]\n","107 [D loss: 0.763034, acc.: 45.75%] [G loss: 0.924553]\n","108 [D loss: 0.776145, acc.: 43.90%] [G loss: 0.915644]\n","109 [D loss: 0.783082, acc.: 44.24%] [G loss: 0.904539]\n","110 [D loss: 0.776558, acc.: 44.14%] [G loss: 0.894810]\n","111 [D loss: 0.773793, acc.: 45.17%] [G loss: 0.876157]\n","112 [D loss: 0.786686, acc.: 43.80%] [G loss: 0.875214]\n","113 [D loss: 0.769027, acc.: 46.53%] [G loss: 0.865354]\n","114 [D loss: 0.788446, acc.: 43.46%] [G loss: 0.900624]\n","115 [D loss: 0.770766, acc.: 44.38%] [G loss: 0.907588]\n","116 [D loss: 0.792413, acc.: 43.21%] [G loss: 0.883696]\n","117 [D loss: 0.781040, acc.: 43.99%] [G loss: 0.884759]\n","118 [D loss: 0.779886, acc.: 43.90%] [G loss: 0.888570]\n","119 [D loss: 0.765480, acc.: 46.09%] [G loss: 0.877056]\n","120 [D loss: 0.742733, acc.: 47.90%] [G loss: 0.908741]\n","121 [D loss: 0.771834, acc.: 45.95%] [G loss: 0.898848]\n","122 [D loss: 0.758269, acc.: 47.90%] [G loss: 0.884022]\n","123 [D loss: 0.770530, acc.: 44.58%] [G loss: 0.886204]\n","124 [D loss: 0.756935, acc.: 47.07%] [G loss: 0.886207]\n","125 [D loss: 0.790018, acc.: 41.85%] [G loss: 0.893024]\n","126 [D loss: 0.776450, acc.: 45.51%] [G loss: 0.896526]\n","127 [D loss: 0.779154, acc.: 42.77%] [G loss: 0.893286]\n","128